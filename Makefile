.PHONY: help up down restart logs ps clean test ci build-images init-db run-dbt seed-data

# Default target
.DEFAULT_GOAL := help

# Colors for terminal output
BLUE := \033[0;34m
GREEN := \033[0;32m
YELLOW := \033[1;33m
RED := \033[0;31m
NC := \033[0m # No Color

##@ General

help: ## Display this help message
	@echo "$(BLUE)DevScout Elite Platform - Data Engineering Showcase$(NC)"
	@echo ""
	@echo "$(GREEN)Available targets:$(NC)"
	@awk 'BEGIN {FS = ":.*##"; printf "\n"} /^[a-zA-Z_-]+:.*?##/ { printf "  $(YELLOW)%-20s$(NC) %s\n", $$1, $$2 } /^##@/ { printf "\n$(BLUE)%s$(NC)\n", substr($$0, 5) } ' $(MAKEFILE_LIST)

##@ Docker Operations

up: ## Start all services (main command to launch platform)
	@echo "$(BLUE)ğŸš€ Starting DevScout Elite Platform...$(NC)"
	@docker-compose up -d
	@echo "$(GREEN)âœ… All services started!$(NC)"
	@echo ""
	@echo "$(YELLOW)ğŸ“ Access Points:$(NC)"
	@echo "  â€¢ Airflow UI:    http://localhost:8080 (airflow/airflow)"
	@echo "  â€¢ MinIO Console: http://localhost:9001 (minioadmin/minioadmin)"
	@echo "  â€¢ Spark Master:  http://localhost:8089"
	@echo "  â€¢ Weaviate:      http://localhost:8081"
	@echo "  â€¢ Dashboard:     http://localhost:8501"
	@echo "  â€¢ API:           http://localhost:8000/docs"
	@echo "  â€¢ Grafana:       http://localhost:3001 (admin/admin)"
	@echo "  â€¢ Jupyter:       http://localhost:8888 (token: devscout)"
	@echo ""
	@echo "$(GREEN)Wait 60 seconds for all services to initialize, then run 'make ci'$(NC)"

down: ## Stop all services
	@echo "$(RED)â¹ï¸  Stopping all services...$(NC)"
	@docker-compose down

restart: ## Restart all services
	@echo "$(YELLOW)ğŸ”„ Restarting services...$(NC)"
	@docker-compose restart

logs: ## View logs from all services
	@docker-compose logs -f

ps: ## Show running containers
	@docker-compose ps

clean: ## Stop services and remove all volumes (âš ï¸  DESTRUCTIVE)
	@echo "$(RED)âš ï¸  WARNING: This will delete all data!$(NC)"
	@echo "Press Ctrl+C to cancel or Enter to continue..."
	@read
	@docker-compose down -v
	@echo "$(GREEN)âœ… Cleaned successfully$(NC)"

##@ Development

build-images: ## Build custom Docker images
	@echo "$(BLUE)ğŸ”¨ Building custom Docker images...$(NC)"
	@docker-compose build

init-db: ## Initialize databases and create schemas
	@echo "$(BLUE)ğŸ“Š Initializing databases...$(NC)"
	@docker-compose exec postgres psql -U devscout -d devscout_dw -f /docker-entrypoint-initdb.d/init.sh
	@echo "$(GREEN)âœ… Database initialized$(NC)"

run-dbt: ## Run dbt transformations (Bronze â†’ Silver â†’ Gold)
	@echo "$(BLUE)ğŸ”„ Running dbt transformations...$(NC)"
	@docker-compose exec dbt dbt run --profiles-dir . --project-dir .
	@docker-compose exec dbt dbt test --profiles-dir . --project-dir .
	@echo "$(GREEN)âœ… dbt run completed$(NC)"

seed-data: ## Load sample data for testing
	@echo "$(BLUE)ğŸŒ± Seeding sample data...$(NC)"
	@python scripts/seed_sample_data.py
	@echo "$(GREEN)âœ… Sample data loaded$(NC)"

##@ Testing & Quality

e2e-test: ## Run end-to-end tests (complete platform test)
	@echo "$(BLUE)ğŸ¯ Running end-to-end tests...$(NC)"
	@python tests/e2e_test.py

upload-sample-data: ## Upload sample resumes to MinIO
	@echo "$(BLUE)ğŸ“¤ Uploading sample data...$(NC)"
	@python tests/upload_sample_data.py

quick-test: ## Quick validation of core services
	@echo "$(BLUE)âš¡ Running quick tests...$(NC)"
	@curl -s http://localhost:8000/health && echo "$(GREEN)âœ… API is healthy$(NC)" || echo "$(RED)âŒ API is down$(NC)"
	@curl -s http://localhost:8080/health && echo "$(GREEN)âœ… Airflow is healthy$(NC)" || echo "$(RED)âŒ Airflow is down$(NC)"

ci: ## Run all tests and quality checks (run this after setup)
	@echo "$(BLUE)ğŸ§ª Running CI checks...$(NC)"
	@echo ""
	@echo "$(YELLOW)1ï¸âƒ£  Unit Tests$(NC)"
	@docker-compose exec -T dbt pytest /usr/app/tests/unit -v
	@echo ""
	@echo "$(YELLOW)2ï¸âƒ£  Data Quality Tests$(NC)"
	@python scripts/data_quality_check.py
	@echo ""
	@echo "$(YELLOW)3ï¸âƒ£  Integration Tests$(NC)"
	@docker-compose exec -T dbt pytest /usr/app/tests/integration -v
	@echo ""
	@echo "$(GREEN)âœ… All checks passed!$(NC)"

test: ## Run unit tests only
	@echo "$(BLUE)ğŸ§ª Running unit tests...$(NC)"
	@docker-compose exec dbt pytest /usr/app/tests/unit -v

test-integration: ## Run integration tests
	@echo "$(BLUE)ğŸ”— Running integration tests...$(NC)"
	@docker-compose exec dbt pytest /usr/app/tests/integration -v

test-coverage: ## Generate test coverage report
	@echo "$(BLUE)ğŸ“Š Generating coverage report...$(NC)"
	@docker-compose exec dbt pytest /usr/app/tests --cov=scripts --cov-report=html
	@echo "$(GREEN)âœ… Coverage report generated at htmlcov/index.html$(NC)"

lint: ## Run code linting
	@echo "$(BLUE)ğŸ” Running linters...$(NC)"
	@docker-compose exec dbt black /usr/app --check
	@docker-compose exec dbt flake8 /usr/app
	@docker-compose exec dbt mypy /usr/app

format: ## Auto-format code
	@echo "$(BLUE)âœ¨ Formatting code...$(NC)"
	@docker-compose exec dbt black /usr/app
	@docker-compose exec dbt isort /usr/app

##@ Data Pipelines

trigger-resume-pipeline: ## Manually trigger resume processing DAG
	@echo "$(BLUE)â–¶ï¸  Triggering resume pipeline...$(NC)"
	@curl -X POST "http://localhost:8080/api/v1/dags/resume_etl_v1/dagRuns" \
		-H "Content-Type: application/json" \
		-u "airflow:airflow" \
		-d '{"conf":{}}'

trigger-github-pipeline: ## Manually trigger GitHub enrichment DAG
	@echo "$(BLUE)â–¶ï¸  Triggering GitHub pipeline...$(NC)"
	@curl -X POST "http://localhost:8080/api/v1/dags/github_ingestion_v1/dagRuns" \
		-H "Content-Type: application/json" \
		-u "airflow:airflow" \
		-d '{"conf":{}}'

run-streaming: ## Start Kafka streaming job
	@echo "$(BLUE)ğŸŒŠ Starting Kafka streaming consumer...$(NC)"
	@docker-compose exec spark-master spark-submit \
		--master spark://spark-master:7077 \
		--packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0 \
		/opt/spark-jobs/streaming/coding_events_consumer.py

##@ Monitoring

health-check: ## Check health of all services
	@echo "$(BLUE)â¤ï¸  Checking service health...$(NC)"
	@echo ""
	@echo "$(YELLOW)Postgres:$(NC) $$(docker-compose exec -T postgres pg_isready -U devscout && echo 'âœ… Healthy' || echo 'âŒ Down')"
	@echo "$(YELLOW)MinIO:$(NC)    $$(curl -s http://localhost:9000/minio/health/live && echo 'âœ… Healthy' || echo 'âŒ Down')"
	@echo "$(YELLOW)Kafka:$(NC)    $$(docker-compose exec -T kafka kafka-broker-api-versions --bootstrap-server localhost:9092 > /dev/null 2>&1 && echo 'âœ… Healthy' || echo 'âŒ Down')"
	@echo "$(YELLOW)Airflow:$(NC)  $$(curl -s http://localhost:8080/health && echo 'âœ… Healthy' || echo 'âŒ Down')"

stats: ## Show resource usage statistics
	@echo "$(BLUE)ğŸ“Š Resource Usage:$(NC)"
	@docker stats --no-stream --format "table {{.Container}}\t{{.CPUPerc}}\t{{.MemUsage}}"

##@ Utilities

shell-airflow: ## Open shell in Airflow container
	@docker-compose exec airflow-webserver bash

shell-spark: ## Open shell in Spark Master container
	@docker-compose exec spark-master bash

shell-dbt: ## Open shell in dbt container
	@docker-compose exec dbt bash

shell-postgres: ## Open PostgreSQL CLI
	@docker-compose exec postgres psql -U devscout -d devscout_dw

backup-db: ## Backup PostgreSQL database
	@echo "$(BLUE)ğŸ’¾ Backing up database...$(NC)"
	@docker-compose exec -T postgres pg_dump -U devscout devscout_dw > backup_$$(date +%Y%m%d_%H%M%S).sql
	@echo "$(GREEN)âœ… Backup complete$(NC)"

##@ Documentation

docs: ## Generate project documentation
	@echo "$(BLUE)ğŸ“š Generating documentation...$(NC)"
	@docker-compose exec dbt dbt docs generate --profiles-dir . --project-dir .
	@docker-compose exec dbt dbt docs serve --port 8082
	@echo "$(GREEN)âœ… Documentation available at http://localhost:8082$(NC)"

architecture-diagram: ## Generate architecture diagram
	@echo "$(BLUE)ğŸ—ï¸  Generating architecture diagram...$(NC)"
	@python scripts/generate_architecture_diagram.py
	@echo "$(GREEN)âœ… Diagram saved to docs/architecture.png$(NC)"
